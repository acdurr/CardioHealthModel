{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Creating Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "## Creating a function transformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "## For Column Transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "\n",
    "## For preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "## For missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## Getting the recall score on our train set\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "## Getting the accuracy score on train set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## Getting the classification report from our train set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## Gridsearch CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "## Imbalanced pipeline and SMOTE\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "## Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing the shape and head\n",
    "def head(df,shape_only=False):\n",
    "    print(df.shape)\n",
    "\n",
    "    if shape_only:\n",
    "        return\n",
    "    else:\n",
    "        return df.head()\n",
    "\n",
    "## for EDA of categorical values\n",
    "def eda_bivariate_categorical(df,column,target):\n",
    "\n",
    "    fig,ax = plt.subplots(figsize = (9,8))\n",
    "\n",
    "    color = 'Set2'\n",
    "\n",
    "    palette_color = sns.color_palette(color)\n",
    "\n",
    "    ax = sns.countplot(x = column, data=df, hue=target,palette=color,order = df[column].value_counts().index)\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "    offset = df[column].value_counts().max() * 0.005\n",
    "\n",
    "    list_bars = df.groupby([column,target])[column].agg(['count']).unstack().fillna(0).values\n",
    "\n",
    "    patches = ax.patches\n",
    "    bars_pos = 0\n",
    "\n",
    "    for i in range(df[target].nunique()):\n",
    "        for j in range(df[column].nunique()):\n",
    "            list_bars_col = list_bars[j] \n",
    "            total_sum = list_bars_col.sum()\n",
    "            value = list_bars_col[i]\n",
    "\n",
    "            percentage = value / total_sum\n",
    "\n",
    "            if percentage == 0:\n",
    "                bars_pos += 1\n",
    "                continue\n",
    "            else:\n",
    "                x = patches[bars_pos].get_x() + patches[j].get_width()/2\n",
    "                y = patches[bars_pos].get_height() + offset\n",
    "                ax.annotate('{:.1f}%'.format(percentage*100), (x, y), ha='center')\n",
    "                bars_pos += 1\n",
    "    plt.show()\n",
    "\n",
    "## Function that plots numerical variables into histogram and violin plot\n",
    "def eda_bivariate_numerical(data,column,target,color,\n",
    "                    figsize=(12,6),\n",
    "                    # save=True,\n",
    "                    val=0,\n",
    "                    target_type = 'Numerical'):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    cmap = sns.color_palette(color)\n",
    "    val = val\n",
    "\n",
    "    for i in range(1):\n",
    "        for j in range(2):\n",
    "            if j==0:\n",
    "                    sns.histplot(data = data,x=data[column],hue=target,\n",
    "                                bins=50,kde=True,palette=color,ax=axes[j])\n",
    "                    axes[j].set(xlabel=None)\n",
    "                    axes[j].grid(False)\n",
    "            elif j==1:\n",
    "                sns.boxplot(data = data,x=data[column],y = target, ax=axes[j], palette=color,orient='h',\n",
    "                )\n",
    "                axes[j].set(xlabel=None)\n",
    "                axes[j].grid(False)\n",
    "                val += 1\n",
    "                plt.tight_layout()\n",
    "            if target_type == 'Numerical':\n",
    "                plt.suptitle(column)\n",
    "            else:\n",
    "                plt.suptitle(f'{column} vs. {target}')\n",
    "    plt.show()\n",
    "    \n",
    "    # path = 'Figures\\\\Numerical\\\\'\n",
    "    # if save:\n",
    "    #     plt.savefig(f\"{path}{column}.pdf\",dpi=1000)\n",
    "\n",
    "\n",
    "## print text to see the font\n",
    "def print_text(text):\n",
    "    fig, ax = plt.subplots(figsize=(6, 1), facecolor=\"#eefade\")\n",
    "    ax.text(0.5, 0.5, text, ha='center', va='center', size=40)\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned.csv')\n",
    "target = 'Heart Disease'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating numerical and categorical columns\n",
    "numerical = df.select_dtypes(include=['float64']).columns.sort_values()\n",
    "categorical = df.select_dtypes(include=['object']).columns.sort_values()\n",
    "\n",
    "## Printing the length of numerical and categorical. The total length should have\n",
    "## the same length as our dataframe\n",
    "print(f'There are {len(categorical)} Categorical variables')\n",
    "print(f'There are {len(numerical)} Numerical variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.sort_values()\n",
    "\n",
    "## Showing the descriptions of numerical variables\n",
    "print('')\n",
    "num_describe = df.describe().T\n",
    "num_describe_table = num_describe.loc[:,['mean', 'std', '25%', '50%', '75%']]\n",
    "print(num_describe_table)\n",
    "\n",
    "## Showing the descriptions of categorical variables\n",
    "print('')\n",
    "object_describe_table = df.describe(include=object)\n",
    "print(object_describe_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in categorical:\n",
    "        if i == target:\n",
    "            continue\n",
    "\n",
    "        if df[i].nunique() > 15:\n",
    "            print(f'column {i} has many unique values n = {df[i].nunique()} and will not be plotted')\n",
    "            print('=======================================================')\n",
    "            continue\n",
    "\n",
    "        if i in df.columns:\n",
    "            print(f'{i} vs. {target}')\n",
    "            eda_bivariate_categorical(df,i,target)\n",
    "            print('=======================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Processing\n",
    "\n",
    "Change Yes/No Heart Disease Responses to 0 and 1 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Heart Disease'] = df['Heart Disease'].map({'No':0,'Yes':1})\n",
    "print('')\n",
    "print(df['Heart Disease'].value_counts())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into testing set and training set. We are using stratify on the set split to maintain ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,test = train_test_split(df, test_size=0.2,random_state=22,stratify=df['Heart Disease'])\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes = train['Heart Disease'].value_counts()[0]/len(train['Heart Disease'])*100\n",
    "no = train['Heart Disease'].value_counts()[1]/len(train['Heart Disease'])*100\n",
    "print('Train Set')\n",
    "print(f'ratio of people with heart disease to total is {yes}')\n",
    "print(f'ratio of people that dont have heart disease to total is {no}')\n",
    "print('')\n",
    "\n",
    "yes = test['Heart Disease'].value_counts()[0]/len(test['Heart Disease'])*100\n",
    "no = test['Heart Disease'].value_counts()[1]/len(test['Heart Disease'])*100\n",
    "print('Test Set')\n",
    "print(f'ratio of people with heart disease to total is {yes}')\n",
    "print(f'ratio of people that dont have heart disease to total is {no}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the X and y variables in the train set\n",
    "X_train = train.drop(\"Heart Disease\", axis=1)\n",
    "y_train = train[\"Heart Disease\"].copy()\n",
    "\n",
    "## Splitting the X and y variables in the test set\n",
    "X_test = test.drop(\"Heart Disease\", axis=1)\n",
    "y_test = test[\"Heart Disease\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = make_pipeline(OneHotEncoder(handle_unknown='ignore',drop='first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = make_pipeline(\n",
    "                             FunctionTransformer(np.log1p,feature_names_out='one-to-one'),\n",
    "                             StandardScaler()\n",
    "                            )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Age Category Pipeline\n",
    "agecat_pipeline = make_pipeline(\n",
    "        OrdinalEncoder()\n",
    ")\n",
    "\n",
    "## General Health Pipeline\n",
    "genhealth_pipeline = make_pipeline(\n",
    "        OrdinalEncoder(categories=[['Poor','Fair','Good','Very Good','Excellent']])\n",
    ")\n",
    "\n",
    "## Checkup Pipeline\n",
    "checkup_pipeline = make_pipeline(\n",
    "        OrdinalEncoder(categories=[['Within the past year','Within the past 2 years','Within the past 5 years','5 or more years ago','Never']])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe_col = numerical\n",
    "\n",
    "cat_pipe_col = ['Arthritis', 'Depression', 'Diabetes',\n",
    "       'Exercise', 'Other Cancer', 'Sex',\n",
    "       'Skin Cancer', 'Smoking History']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining all the pipelines and creating a main pipeline to enter all the data\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('Categorical', cat_pipeline,   cat_pipe_col),\n",
    "    ('Age',agecat_pipeline,['Age']),\n",
    "    ('Checkup',checkup_pipeline,['Checkup']),\n",
    "    ('General Health',genhealth_pipeline,['General Health']),\n",
    "    ('Numerical',   num_pipeline,  num_pipe_col),\n",
    "],remainder='passthrough')\n",
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using preprocessing pipeline\n",
    "print('Shape before the preprocessing:')\n",
    "print(X_train.shape)\n",
    "\n",
    "train_preprocessed = preprocessing.fit_transform(X_train)\n",
    "\n",
    "print('Shape after the preprocessing:')\n",
    "print(train_preprocessed.shape)\n",
    "print(train_preprocessed)\n",
    "\n",
    "dump(preprocessing, \"preprocessing.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = {\n",
    "#    'Logistic_Regression':LogisticRegression(max_iter=10000,random_state=22),\n",
    "#    'Decision Tree':DecisionTreeClassifier(random_state=22),\n",
    "#    'Random_Forest':RandomForestClassifier(n_estimators=100,random_state=22),\n",
    "#    'K-Nearest_Neighbor':KNeighborsClassifier(),\n",
    "#    'GaussianNB':GaussianNB(),\n",
    "#    'MLP_Classifier':MLPClassifier(random_state=22, max_iter=10000)\n",
    "#}\n",
    "\n",
    "models = {'K-Nearest_Neighbor':KNeighborsClassifier()}\n",
    "\n",
    "scores_dict = {}\n",
    "\n",
    "report_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name,model in models.items():\n",
    "    model_pipeline = make_pipeline(preprocessing,\n",
    "                              SMOTE(random_state=22),\n",
    "                              model  \n",
    "                                )\n",
    "    scores = cross_val_score(model_pipeline, \n",
    "                            X_train, \n",
    "                            y_train, \n",
    "                            scoring='f1', \n",
    "                            cv=kf,\n",
    "                            verbose=1,\n",
    "                            n_jobs=-1,\n",
    "                            )\n",
    "    model_score_mean = np.mean(scores)\n",
    "    scores_dict[model_name] = model_score_mean\n",
    "    print('------------------------------------------------------------')\n",
    "    print(f'The score for {model_name} is {model_score_mean}')\n",
    "\n",
    "    ## fitting the pipeline for classification report\n",
    "    model_pipeline.fit(X_train,y_train)\n",
    "    \n",
    "    dump(model_pipeline, 'k-nearest-model.joblib')\n",
    "    \n",
    "    prediction = model_pipeline.predict(X_train)\n",
    "    print(prediction)\n",
    "\n",
    "    report = classification_report(y_train, prediction, output_dict=True)\n",
    "    report_dict[model_name] = report\n",
    "    print('')\n",
    "    print(f'This is the classification report for {model_name}:')\n",
    "    report_df = pd.DataFrame(report).T\n",
    "    print(report_df)\n",
    "    print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = list(X_train.columns)\n",
    "dump(model_columns, 'cols.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
